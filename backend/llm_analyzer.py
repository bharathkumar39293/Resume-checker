import os
import json
from transformers import pipeline

# Initialize a local Hugging Face text generation pipeline
# 'distilgpt2' is a small, fast model for text generation.
# For better quality, consider larger models, but they require more resources.
text_generator = pipeline("text-generation", model="distilgpt2")

def analyze_match(resume_text, jd_text):
    prompt = f"""Analyze the following resume and job description. 

Resume:
{resume_text}

Job Description:
{jd_text}

On a scale of 0-100, how well does the resume match the job requirements? 
Identify three missing elements and suggest improvements in direct, actionable sentences. 

Output in the following JSON format:
{{
    "match_score": <integer 0-100>,
    "missing_elements": [
        {{"element": "<missing_element_1>", "suggestion": "<suggestion_1>"}},
        {{"element": "<missing_element_2>", "suggestion": "<suggestion_2>"}},
        {{"element": "<missing_element_3>", "suggestion": "<suggestion_3>"}}
    ]
}}
"""

    try:
        # Generate text using the local LLM
        # max_new_tokens controls the length of the generated output
        # We'll set it to a reasonable length for a structured JSON response.
        # num_return_sequences=1 ensures we get only one response.
        # We need to manually parse the JSON output from the raw text generated by a simpler model.
        raw_output = text_generator(prompt, max_new_tokens=500, num_return_sequences=1)[0]['generated_text']
        
        # The model might repeat the prompt, so we try to extract the JSON part.
        # This is a simple heuristic; more robust parsing might be needed for complex outputs.
        json_start = raw_output.find('{\n    "match_score"')
        if json_start != -1:
            json_string = raw_output[json_start:]
            # Clean up any trailing text after the JSON block
            json_string = json_string.split('}\n')[0] + '}'
            try:
                llm_output = json.loads(json_string)
                return json.dumps(llm_output) # Return as JSON string
            except json.JSONDecodeError:
                print(f"Warning: Could not decode JSON from LLM output: {json_string}")
                # Fallback for when JSON parsing fails
                return json.dumps({"match_score": 0, "missing_elements": [{"element": "N/A", "suggestion": "Could not parse LLM output."}]})
        else:
            print(f"Warning: No JSON structure found in LLM output: {raw_output}")
            return json.dumps({"match_score": 0, "missing_elements": [{"element": "N/A", "suggestion": "LLM output not in expected format."}]})

    except Exception as e:
        raise Exception(f"Error calling local LLM for analysis: {str(e)}")

def generate_feedback(resume_text, jd_text):
    prompt = f"""For this resume, list specific changes required to maximize fit for the uploaded job description. 
Focus on skills, certifications, and project additions.

Resume:
{resume_text}

Job Description:
{jd_text}

Output in the following JSON format:
{{
    "feedback": [
        {{"area": "<area_1>", "suggestion": "<suggestion_1>"}},
        {{"area": "<area_2>", "suggestion": "<suggestion_2>"}},
        {{"area": "<area_3>", "suggestion": "<suggestion_3>"}}
    ]
}}
"""

    try:
        raw_output = text_generator(prompt, max_new_tokens=500, num_return_sequences=1)[0]['generated_text']

        json_start = raw_output.find('{\n    "feedback"')
        if json_start != -1:
            json_string = raw_output[json_start:]
            json_string = json_string.split('}\n')[0] + '}'
            try:
                llm_output = json.loads(json_string)
                return json.dumps(llm_output)
            except json.JSONDecodeError:
                print(f"Warning: Could not decode JSON from LLM output: {json_string}")
                return json.dumps({"feedback": [{"area": "N/A", "suggestion": "Could not parse LLM feedback."}]})
        else:
            print(f"Warning: No JSON structure found in LLM output: {raw_output}")
            return json.dumps({"feedback": [{"area": "N/A", "suggestion": "LLM feedback not in expected format."}]})
    except Exception as e:
        raise Exception(f"Error calling local LLM for feedback generation: {str(e)}")

if __name__ == "__main__":
    # Example usage with local LLM
    sample_resume = "I am a software developer with experience in Python, Flask, and building REST APIs. I have worked on web applications and have some knowledge of databases."
    sample_jd = "We are looking for a Senior Python Developer with strong expertise in FastAPI, microservices, and cloud deployments (AWS/Azure). Experience with NoSQL databases and CI/CD pipelines is a plus."

    print("Analyzing match using local LLM...")
    try:
        llm_result = analyze_match(sample_resume, sample_jd)
        print("LLM Analysis:", llm_result)

        print("\nGenerating feedback using local LLM...")
        feedback_result = generate_feedback(sample_resume, sample_jd)
        print("LLM Feedback:", feedback_result)

    except Exception as e:
        print(f"Error: {e}")
